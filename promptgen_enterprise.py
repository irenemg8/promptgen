"""
PromptGen Enterprise - Sistema Avanzado de Mejora de Prompts
===========================================================

Sistema empresarial robusto para la mejora iterativa de prompts utilizando:
- Modelos reales de Hugging Face (sin mockups)
- Memoria contextual para aprendizaje iterativo
- Algoritmos de mejora progresiva
- M√©tricas de calidad avanzadas
- Procesamiento inteligente en espa√±ol

Autor: Senior DevOps Engineer
Versi√≥n: 2.0.0 Enterprise
"""

import os
import torch
import warnings
import re
import time
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from transformers import (
    pipeline, 
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig
)
import nltk
import spacy
from sentence_transformers import SentenceTransformer, util
import numpy as np

# Configuraci√≥n de logging empresarial
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Suprimir warnings no cr√≠ticos
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Inicializaci√≥n de recursos NLP
try:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nlp = spacy.load("es_core_news_sm")
except Exception as e:
    logger.warning(f"Error cargando recursos NLP: {e}")
    nlp = None

@dataclass
class PromptIteration:
    """Clase para almacenar informaci√≥n de cada iteraci√≥n del prompt"""
    iteration: int
    original_prompt: str
    improved_prompt: str
    quality_score: float
    improvements_made: List[str]
    feedback_applied: List[str]
    timestamp: datetime
    model_used: str
    processing_time: float
    
@dataclass
class QualityMetrics:
    """M√©tricas avanzadas de calidad del prompt"""
    completeness: float
    clarity: float
    specificity: float
    structure: float
    coherence: float
    actionability: float
    overall_score: float
    improvement_potential: float

class PromptMemorySystem:
    """Sistema de memoria contextual para aprendizaje iterativo"""
    
    def __init__(self):
        self.iteration_history: Dict[str, List[PromptIteration]] = {}
        self.successful_patterns: Dict[str, List[str]] = {}
        self.failed_patterns: Dict[str, List[str]] = {}
        self.quality_trends: Dict[str, List[float]] = {}
        
    def add_iteration(self, session_id: str, iteration: PromptIteration):
        """A√±ade una nueva iteraci√≥n al historial"""
        if session_id not in self.iteration_history:
            self.iteration_history[session_id] = []
            self.quality_trends[session_id] = []
            
        self.iteration_history[session_id].append(iteration)
        self.quality_trends[session_id].append(iteration.quality_score)
        
        # Analizar patrones exitosos/fallidos
        self._analyze_patterns(session_id, iteration)
        
    def _analyze_patterns(self, session_id: str, iteration: PromptIteration):
        """Analiza patrones exitosos y fallidos para aprendizaje"""
        if len(self.quality_trends[session_id]) < 2:
            return
            
        current_score = iteration.quality_score
        previous_score = self.quality_trends[session_id][-2]
        
        if current_score > previous_score:
            # Patr√≥n exitoso
            for improvement in iteration.improvements_made:
                if session_id not in self.successful_patterns:
                    self.successful_patterns[session_id] = []
                self.successful_patterns[session_id].append(improvement)
        else:
            # Patr√≥n fallido
            for improvement in iteration.improvements_made:
                if session_id not in self.failed_patterns:
                    self.failed_patterns[session_id] = []
                self.failed_patterns[session_id].append(improvement)
                
    def get_learning_insights(self, session_id: str) -> Dict[str, Any]:
        """Obtiene insights de aprendizaje para mejorar futuras iteraciones"""
        if session_id not in self.iteration_history:
            return {}
            
        history = self.iteration_history[session_id]
        trends = self.quality_trends[session_id]
        
        return {
            'total_iterations': len(history),
            'quality_trend': 'improving' if len(trends) > 1 and trends[-1] > trends[0] else 'declining',
            'best_score': max(trends) if trends else 0,
            'successful_patterns': self.successful_patterns.get(session_id, []),
            'failed_patterns': self.failed_patterns.get(session_id, []),
            'average_improvement': np.mean(np.diff(trends)) if len(trends) > 1 else 0
        }

class EnterpriseModelManager:
    """Gestor empresarial de modelos de Hugging Face"""
    
    def __init__(self):
        self.model_cache: Dict[str, Any] = {}
        self.model_configs = {
            "gpt2": {
                "name": "gpt2",
                "type": "causal",
                "max_tokens": 150,
                "temperature": 0.8,
                "top_p": 0.9
            },
            "distilgpt2": {
                "name": "distilgpt2", 
                "type": "causal",
                "max_tokens": 120,
                "temperature": 0.7,
                "top_p": 0.85
            },
            "gpt-neo-125m": {
                "name": "EleutherAI/gpt-neo-125M",
                "type": "causal", 
                "max_tokens": 180,
                "temperature": 0.75,
                "top_p": 0.9
            },
            "t5-small": {
                "name": "google/t5-v1_1-small",
                "type": "seq2seq",
                "max_tokens": 100,
                "temperature": 0.8,
                "top_p": 0.9
            }
        }
        
    def load_model(self, model_key: str) -> Optional[Any]:
        """Carga un modelo de forma empresarial con manejo de errores robusto"""
        if model_key in self.model_cache:
            return self.model_cache[model_key]
            
        if model_key not in self.model_configs:
            logger.error(f"Modelo {model_key} no configurado")
            return None
            
        config = self.model_configs[model_key]
        logger.info(f"üîÑ Cargando modelo empresarial: {config['name']}")
        
        start_time = time.time()
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config['name'], 
                trust_remote_code=True
            )
            
            if config['type'] == 'seq2seq':
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    config['name'],
                    trust_remote_code=True
                )
                pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    config['name'],
                    trust_remote_code=True
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    model.config.pad_token_id = model.config.eos_token_id
                    
                pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
                
            self.model_cache[model_key] = {
                'pipeline': pipe,
                'config': config,
                'tokenizer': tokenizer
            }
            
            load_time = time.time() - start_time
            logger.info(f"‚úÖ Modelo {model_key} cargado en {load_time:.2f}s")
            
            return self.model_cache[model_key]
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo {model_key}: {e}")
            return None
            
    def generate_text(self, model_key: str, prompt: str, task_type: str = "improve") -> Optional[str]:
        """Genera texto usando el modelo especificado con configuraci√≥n empresarial"""
        model_data = self.load_model(model_key)
        if not model_data:
            return None
            
        pipe = model_data['pipeline']
        config = model_data['config']
        
        # Crear prompt contextual seg√∫n el tipo de tarea
        contextual_prompt = self._create_contextual_prompt(prompt, task_type, config['type'])
        
        logger.info(f"ü§ñ Generando con {model_key} para tarea: {task_type}")
        start_time = time.time()
        
        try:
            if config['type'] == 'seq2seq':
                result = pipe(
                    contextual_prompt,
                    max_length=config['max_tokens'],
                    temperature=config['temperature'],
                    top_p=config['top_p'],
                    do_sample=True,
                    num_return_sequences=1
                )
                generated_text = result[0]['generated_text']
            else:
                result = pipe(
                    contextual_prompt,
                    max_new_tokens=config['max_tokens'],
                    temperature=config['temperature'],
                    top_p=config['top_p'],
                    do_sample=True,
                    num_return_sequences=1,
                    pad_token_id=pipe.tokenizer.eos_token_id,
                    repetition_penalty=1.2
                )
                generated_text = result[0]['generated_text']
                
                # Remover el prompt original para modelos causales
                if generated_text.startswith(contextual_prompt):
                    generated_text = generated_text[len(contextual_prompt):].strip()
                    
            generation_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è Generaci√≥n completada en {generation_time:.2f}s")
            
            return self._post_process_spanish(generated_text)
            
        except Exception as e:
            logger.error(f"‚ùå Error en generaci√≥n: {e}")
            return None
            
    def _create_contextual_prompt(self, prompt: str, task_type: str, model_type: str) -> str:
        """Crea prompts contextuales optimizados para cada tarea y modelo"""
        if task_type == "improve":
            if model_type == "seq2seq":
                return f"Mejora este prompt: {prompt}"
            else:
                return f"Prompt original: {prompt}\nPrompt mejorado:"
                
        elif task_type == "feedback":
            if model_type == "seq2seq":
                return f"Analiza y da feedback sobre: {prompt}"
            else:
                return f"An√°lisis del prompt '{prompt}':\n1."
                
        elif task_type == "ideas":
            if model_type == "seq2seq":
                return f"Genera ideas para: {prompt}"
            else:
                return f"Ideas para mejorar '{prompt}':\n-"
                
        return prompt
        
    def _post_process_spanish(self, text: str) -> str:
        """Post-procesamiento para asegurar salida en espa√±ol de calidad"""
        if not text:
            return ""
            
        # Limpiar texto
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Filtrar solo oraciones en espa√±ol
        sentences = re.split(r'[.!?]+', text)
        spanish_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # Muy corto
                continue
                
            # Verificar si es espa√±ol
            if self._is_spanish_sentence(sentence):
                spanish_sentences.append(sentence)
                
        result = '. '.join(spanish_sentences)
        if result and not result.endswith('.'):
            result += '.'
            
        return result
        
    def _is_spanish_sentence(self, sentence: str) -> bool:
        """Verifica si una oraci√≥n est√° en espa√±ol"""
        # Palabras claramente no espa√±olas
        non_spanish_words = [
            'the', 'and', 'with', 'from', 'this', 'that', 'have', 'will',
            's√£o', 'para', 'com', 'uma', 'est', 'une', 'avec', 'dans'
        ]
        
        sentence_lower = sentence.lower()
        
        # Si contiene palabras no espa√±olas, rechazar
        if any(word in sentence_lower for word in non_spanish_words):
            return False
            
        # Contar caracteres espa√±oles
        spanish_chars = len(re.findall(r'[a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú]', sentence))
        total_chars = len(re.findall(r'[a-zA-Z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú]', sentence))
        
        if total_chars == 0:
            return False
            
        spanish_ratio = spanish_chars / total_chars
        return spanish_ratio > 0.7

class AdvancedQualityAnalyzer:
    """Analizador avanzado de calidad de prompts con m√©tricas empresariales"""
    
    def __init__(self):
        self.similarity_model = None
        try:
            self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
        except Exception as e:
            logger.warning(f"No se pudo cargar modelo de similitud: {e}")
            
    def analyze_quality(self, prompt: str, context: Dict[str, Any] = None) -> QualityMetrics:
        """An√°lisis avanzado de calidad con m√∫ltiples m√©tricas"""
        
        # M√©tricas b√°sicas
        completeness = self._analyze_completeness(prompt)
        clarity = self._analyze_clarity(prompt)
        specificity = self._analyze_specificity(prompt)
        structure = self._analyze_structure(prompt)
        
        # M√©tricas avanzadas
        coherence = self._analyze_coherence(prompt)
        actionability = self._analyze_actionability(prompt)
        
        # Calcular puntuaci√≥n general
        scores = [completeness, clarity, specificity, structure, coherence, actionability]
        overall_score = sum(scores) / len(scores)
        
        # Calcular potencial de mejora
        improvement_potential = 100 - overall_score
        
        return QualityMetrics(
            completeness=completeness,
            clarity=clarity,
            specificity=specificity,
            structure=structure,
            coherence=coherence,
            actionability=actionability,
            overall_score=overall_score,
            improvement_potential=improvement_potential
        )
        
    def _analyze_completeness(self, prompt: str) -> float:
        """Analiza la completitud del prompt"""
        words = prompt.split()
        word_count = len(words)
        
        # Elementos clave que debe tener un prompt completo
        key_elements = {
            'objetivo': ['crear', 'desarrollar', 'dise√±ar', 'implementar', 'generar'],
            'contexto': ['para', 'dirigido', 'orientado', 'enfocado'],
            'especificidad': ['con', 'que', 'incluye', 'mediante'],
            'audiencia': ['usuario', 'cliente', 'estudiante', 'empresa']
        }
        
        elements_found = 0
        prompt_lower = prompt.lower()
        
        for element_type, keywords in key_elements.items():
            if any(keyword in prompt_lower for keyword in keywords):
                elements_found += 1
                
        # Puntuaci√≥n basada en longitud y elementos clave
        length_score = min(word_count * 2, 60)  # M√°ximo 60 por longitud
        elements_score = (elements_found / len(key_elements)) * 40  # M√°ximo 40 por elementos
        
        return min(length_score + elements_score, 100)
        
    def _analyze_clarity(self, prompt: str) -> float:
        """Analiza la claridad del prompt"""
        words = prompt.split()
        
        # Penalizar palabras vagas
        vague_words = ['cosa', 'algo', 'esto', 'eso', 'bueno', 'normal', 'b√°sico']
        vague_count = sum(1 for word in words if word.lower() in vague_words)
        
        # Penalizar oraciones muy largas
        sentences = re.split(r'[.!?]+', prompt)
        avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / max(len([s for s in sentences if s.strip()]), 1)
        
        # Calcular puntuaci√≥n
        clarity_score = 100
        clarity_score -= vague_count * 10  # -10 por cada palabra vaga
        clarity_score -= max(0, (avg_sentence_length - 15) * 2)  # Penalizar oraciones muy largas
        
        return max(clarity_score, 0)
        
    def _analyze_specificity(self, prompt: str) -> float:
        """Analiza la especificidad del prompt"""
        prompt_lower = prompt.lower()
        
        # Elementos espec√≠ficos
        specific_elements = {
            'tecnologia': ['api', 'rest', 'web', 'app', 'sistema', 'base de datos', 'frontend', 'backend'],
            'funcionalidad': ['login', 'registro', 'busqueda', 'filtro', 'dashboard', 'reporte'],
            'industria': ['educacion', 'salud', 'finanzas', 'comercio', 'logistica'],
            'metricas': ['usuarios', 'tiempo', 'rendimiento', 'escalabilidad']
        }
        
        specificity_score = 0
        for category, terms in specific_elements.items():
            if any(term in prompt_lower for term in terms):
                specificity_score += 25
                
        return min(specificity_score, 100)
        
    def _analyze_structure(self, prompt: str) -> float:
        """Analiza la estructura del prompt"""
        # Verificar estructura b√°sica
        has_subject = bool(re.search(r'\b(crear|desarrollar|dise√±ar|implementar|generar)\b', prompt.lower()))
        has_object = bool(re.search(r'\b(sistema|aplicacion|web|app|plataforma)\b', prompt.lower()))
        has_context = bool(re.search(r'\b(para|dirigido|orientado|enfocado)\b', prompt.lower()))
        
        structure_elements = [has_subject, has_object, has_context]
        structure_score = (sum(structure_elements) / len(structure_elements)) * 100
        
        return structure_score
        
    def _analyze_coherence(self, prompt: str) -> float:
        """Analiza la coherencia del prompt"""
        if not self.similarity_model:
            return 75  # Puntuaci√≥n por defecto si no hay modelo
            
        sentences = re.split(r'[.!?]+', prompt)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) < 2:
            return 90  # Un solo concepto, coherente por defecto
            
        try:
            embeddings = self.similarity_model.encode(sentences)
            similarities = []
            
            for i in range(len(embeddings) - 1):
                sim = util.cos_sim(embeddings[i], embeddings[i + 1])
                similarities.append(float(sim))
                
            avg_similarity = sum(similarities) / len(similarities)
            coherence_score = avg_similarity * 100
            
            return min(max(coherence_score, 0), 100)
            
        except Exception as e:
            logger.warning(f"Error en an√°lisis de coherencia: {e}")
            return 75
            
    def _analyze_actionability(self, prompt: str) -> float:
        """Analiza qu√© tan accionable es el prompt"""
        prompt_lower = prompt.lower()
        
        # Verbos de acci√≥n
        action_verbs = [
            'crear', 'desarrollar', 'dise√±ar', 'implementar', 'generar',
            'construir', 'establecer', 'configurar', 'optimizar', 'mejorar'
        ]
        
        # Elementos concretos
        concrete_elements = [
            'funcionalidad', 'caracteristica', 'modulo', 'componente',
            'interfaz', 'base de datos', 'api', 'sistema'
        ]
        
        action_score = 50 if any(verb in prompt_lower for verb in action_verbs) else 0
        concrete_score = 50 if any(element in prompt_lower for element in concrete_elements) else 0
        
        return action_score + concrete_score

class ProgressiveImprovementEngine:
    """Motor de mejora progresiva que aprende de iteraciones anteriores"""
    
    def __init__(self, model_manager: EnterpriseModelManager, quality_analyzer: AdvancedQualityAnalyzer):
        self.model_manager = model_manager
        self.quality_analyzer = quality_analyzer
        self.memory_system = PromptMemorySystem()
        
    def improve_prompt_iteratively(
        self, 
        original_prompt: str, 
        model_name: str = "gpt2",
        max_iterations: int = 5,
        target_quality: float = 85.0
    ) -> Dict[str, Any]:
        """Mejora un prompt de forma iterativa usando aprendizaje contextual"""
        
        session_id = self._generate_session_id(original_prompt)
        current_prompt = original_prompt
        iterations_data = []
        
        logger.info(f"üöÄ Iniciando mejora iterativa para sesi√≥n: {session_id}")
        
        for iteration in range(max_iterations):
            logger.info(f"üìà Iteraci√≥n {iteration + 1}/{max_iterations}")
            
            start_time = time.time()
            
            # Analizar calidad actual
            current_quality = self.quality_analyzer.analyze_quality(current_prompt)
            
            # Si ya alcanzamos la calidad objetivo, terminar
            if current_quality.overall_score >= target_quality:
                logger.info(f"üéØ Calidad objetivo alcanzada: {current_quality.overall_score:.1f}%")
                break
                
            # Obtener insights de aprendizaje previo
            learning_insights = self.memory_system.get_learning_insights(session_id)
            
            # Generar mejoras inteligentes
            improvements = self._generate_intelligent_improvements(
                current_prompt, 
                current_quality,
                learning_insights,
                model_name
            )
            
            # Aplicar mejoras
            improved_prompt = self._apply_improvements(current_prompt, improvements)
            
            # Validar mejora
            improved_quality = self.quality_analyzer.analyze_quality(improved_prompt)
            
            processing_time = time.time() - start_time
            
            # Crear registro de iteraci√≥n
            iteration_record = PromptIteration(
                iteration=iteration + 1,
                original_prompt=current_prompt,
                improved_prompt=improved_prompt,
                quality_score=improved_quality.overall_score,
                improvements_made=improvements,
                feedback_applied=self._extract_feedback_applied(improvements),
                timestamp=datetime.now(),
                model_used=model_name,
                processing_time=processing_time
            )
            
            # A√±adir a memoria
            self.memory_system.add_iteration(session_id, iteration_record)
            
            # Guardar datos de iteraci√≥n
            iterations_data.append({
                'iteration': iteration + 1,
                'original_prompt': current_prompt,
                'improved_prompt': improved_prompt,
                'quality_before': current_quality.overall_score,
                'quality_after': improved_quality.overall_score,
                'improvement_delta': improved_quality.overall_score - current_quality.overall_score,
                'improvements_made': improvements,
                'processing_time': processing_time
            })
            
            logger.info(f"üìä Calidad: {current_quality.overall_score:.1f}% ‚Üí {improved_quality.overall_score:.1f}%")
            
            # Actualizar prompt actual
            current_prompt = improved_prompt
            
        # Generar reporte final
        final_quality = self.quality_analyzer.analyze_quality(current_prompt)
        learning_insights = self.memory_system.get_learning_insights(session_id)
        
        return {
            'session_id': session_id,
            'original_prompt': original_prompt,
            'final_prompt': current_prompt,
            'initial_quality': self.quality_analyzer.analyze_quality(original_prompt).overall_score,
            'final_quality': final_quality.overall_score,
            'total_improvement': final_quality.overall_score - self.quality_analyzer.analyze_quality(original_prompt).overall_score,
            'iterations_completed': len(iterations_data),
            'iterations_data': iterations_data,
            'learning_insights': learning_insights,
            'quality_metrics': asdict(final_quality)
        }
        
    def improve_iteratively(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Alias para improve_prompt_iteratively para compatibilidad con API"""
        return self.improve_prompt_iteratively(original_prompt=prompt, **kwargs)
        
    def _generate_session_id(self, prompt: str) -> str:
        """Genera un ID √∫nico para la sesi√≥n"""
        return hashlib.md5(f"{prompt}{datetime.now().isoformat()}".encode()).hexdigest()[:8]
        
    def _generate_intelligent_improvements(
        self, 
        prompt: str, 
        quality: QualityMetrics,
        learning_insights: Dict[str, Any],
        model_name: str
    ) -> List[str]:
        """Genera mejoras inteligentes basadas en an√°lisis y aprendizaje previo"""
        
        improvements = []
        
        # Mejoras basadas en m√©tricas de calidad
        if quality.completeness < 70:
            improvements.extend(self._generate_completeness_improvements(prompt))
            
        if quality.clarity < 70:
            improvements.extend(self._generate_clarity_improvements(prompt))
            
        if quality.specificity < 70:
            improvements.extend(self._generate_specificity_improvements(prompt, model_name))
            
        if quality.actionability < 70:
            improvements.extend(self._generate_actionability_improvements(prompt))
            
        # Aplicar aprendizaje de patrones exitosos
        if learning_insights.get('successful_patterns'):
            successful_patterns = learning_insights['successful_patterns']
            # Evitar patrones que ya fallaron
            failed_patterns = learning_insights.get('failed_patterns', [])
            
            for pattern in successful_patterns[-3:]:  # √öltimos 3 patrones exitosos
                if pattern not in failed_patterns:
                    improvements.append(pattern)
                    
        return improvements[:5]  # Limitar a 5 mejoras por iteraci√≥n
        
    def _generate_completeness_improvements(self, prompt: str) -> List[str]:
        """Genera mejoras para completitud"""
        improvements = []
        prompt_lower = prompt.lower()
        
        if 'usuario' not in prompt_lower and 'cliente' not in prompt_lower:
            improvements.append("especificar el tipo de usuarios objetivo")
            
        if not any(tech in prompt_lower for tech in ['web', 'app', 'sistema', 'plataforma']):
            improvements.append("definir la tecnolog√≠a o plataforma espec√≠fica")
            
        if not any(func in prompt_lower for func in ['funcionalidad', 'caracteristica', 'modulo']):
            improvements.append("incluir funcionalidades espec√≠ficas requeridas")
            
        return improvements
        
    def _generate_clarity_improvements(self, prompt: str) -> List[str]:
        """Genera mejoras para claridad"""
        improvements = []
        
        # Detectar palabras vagas
        vague_words = ['cosa', 'algo', 'esto', 'eso', 'bueno', 'normal']
        if any(word in prompt.lower() for word in vague_words):
            improvements.append("reemplazar t√©rminos vagos con descripciones espec√≠ficas")
            
        # Detectar oraciones muy largas
        sentences = re.split(r'[.!?]+', prompt)
        long_sentences = [s for s in sentences if len(s.split()) > 20]
        if long_sentences:
            improvements.append("dividir oraciones largas en conceptos m√°s claros")
            
        return improvements
        
    def _generate_specificity_improvements(self, prompt: str, model_name: str) -> List[str]:
        """Genera mejoras para especificidad usando el modelo"""
        improvements = []
        
        # Usar el modelo para generar sugerencias espec√≠ficas
        specific_prompt = f"Sugiere 3 caracter√≠sticas espec√≠ficas para: {prompt}"
        generated_suggestions = self.model_manager.generate_text(
            model_name, 
            specific_prompt, 
            "ideas"
        )
        
        if generated_suggestions:
            # Extraer sugerencias del texto generado
            suggestions = re.findall(r'[-‚Ä¢]\s*([^.\n]+)', generated_suggestions)
            for suggestion in suggestions[:2]:  # M√°ximo 2 sugerencias
                if len(suggestion.strip()) > 10:
                    improvements.append(f"incluir {suggestion.strip().lower()}")
                    
        return improvements
        
    def _generate_actionability_improvements(self, prompt: str) -> List[str]:
        """Genera mejoras para accionabilidad"""
        improvements = []
        prompt_lower = prompt.lower()
        
        action_verbs = ['crear', 'desarrollar', 'dise√±ar', 'implementar']
        if not any(verb in prompt_lower for verb in action_verbs):
            improvements.append("a√±adir verbo de acci√≥n espec√≠fico (crear, desarrollar, implementar)")
            
        if 'objetivo' not in prompt_lower and 'meta' not in prompt_lower:
            improvements.append("definir el objetivo principal del proyecto")
            
        return improvements
        
    def _apply_improvements(self, original_prompt: str, improvements: List[str]) -> str:
        """Aplica las mejoras al prompt original de forma inteligente"""
        
        improved_prompt = original_prompt
        
        for improvement in improvements:
            if "especificar el tipo de usuarios" in improvement:
                if "usuario" not in improved_prompt.lower():
                    improved_prompt += " dirigido a usuarios profesionales"
                    
            elif "definir la tecnolog√≠a" in improvement:
                if "web" not in improved_prompt.lower() and "app" not in improved_prompt.lower():
                    improved_prompt += " como aplicaci√≥n web moderna"
                    
            elif "incluir funcionalidades" in improvement:
                if "funcionalidad" not in improved_prompt.lower():
                    improved_prompt += " con funcionalidades avanzadas de gesti√≥n"
                    
            elif "reemplazar t√©rminos vagos" in improvement:
                # Reemplazar palabras vagas
                replacements = {
                    'cosa': 'elemento',
                    'algo': 'componente',
                    'esto': 'el sistema',
                    'bueno': 'eficiente'
                }
                for vague, specific in replacements.items():
                    improved_prompt = improved_prompt.replace(vague, specific)
                    
            elif "a√±adir verbo de acci√≥n" in improvement:
                if not any(verb in improved_prompt.lower() for verb in ['crear', 'desarrollar', 'dise√±ar']):
                    improved_prompt = "Desarrollar " + improved_prompt.lower()
                    
            elif "incluir" in improvement:
                # Extraer lo que se debe incluir
                to_include = improvement.replace("incluir ", "")
                improved_prompt += f" que incluya {to_include}"
                
        return improved_prompt.strip()
        
    def _extract_feedback_applied(self, improvements: List[str]) -> List[str]:
        """Extrae el feedback aplicado de las mejoras"""
        feedback = []
        for improvement in improvements:
            if "especificar" in improvement:
                feedback.append("Especificidad mejorada")
            elif "definir" in improvement:
                feedback.append("Definici√≥n a√±adida")
            elif "incluir" in improvement:
                feedback.append("Funcionalidad a√±adida")
            elif "reemplazar" in improvement:
                feedback.append("Claridad mejorada")
                
        return feedback

# Funciones de API para compatibilidad con el sistema existente
def analyze_prompt_quality_bart(prompt: str) -> Dict[str, Any]:
    """Funci√≥n de compatibilidad para an√°lisis de calidad"""
    analyzer = AdvancedQualityAnalyzer()
    quality = analyzer.analyze_quality(prompt)
    
    # Extraer concepto principal
    concept = extract_core_concept_enterprise(prompt)
    project_type = detect_project_type_enterprise(concept)
    
    return {
        'quality_report': f"üìä An√°lisis detallado del prompt ({len(prompt.split())} palabras)\n\n"
                         f"{'‚úÖ' if quality.overall_score >= 80 else '‚ö†Ô∏è' if quality.overall_score >= 60 else '‚ùå'} "
                         f"Calidad general: {quality.overall_score:.0f}% - "
                         f"{'Excelente' if quality.overall_score >= 80 else 'Buena' if quality.overall_score >= 60 else 'Necesita Mejoras'}\n"
                         f"üéØ Tipo de proyecto detectado: {project_type.title()}\n"
                         f"üîë Concepto principal: {concept}\n\n"
                         f"üìà An√°lisis por categor√≠as:\n"
                         f"‚Ä¢ Completitud: {quality.completeness:.0f}%\n"
                         f"‚Ä¢ Claridad: {quality.clarity:.0f}%\n"
                         f"‚Ä¢ Especificidad: {quality.specificity:.0f}%\n"
                         f"‚Ä¢ Estructura: {quality.structure:.0f}%\n"
                         f"‚Ä¢ Coherencia: {quality.coherence:.0f}%\n"
                         f"‚Ä¢ Accionabilidad: {quality.actionability:.0f}%",
        'interpreted_keywords': ', '.join(extract_keywords_enterprise(prompt))
    }

def get_structural_feedback(prompt: str, model_name: str = "gpt2") -> Dict[str, Any]:
    """Funci√≥n de compatibilidad para feedback estructural"""
    model_manager = EnterpriseModelManager()
    analyzer = AdvancedQualityAnalyzer()
    
    quality = analyzer.analyze_quality(prompt)
    concept = extract_core_concept_enterprise(prompt)
    
    # Generar feedback inteligente
    feedback_items = []
    
    if quality.completeness < 70:
        feedback_items.append(f"Define el tipo de usuarios objetivo para el {concept}")
        
    if quality.specificity < 70:
        feedback_items.append(f"Especifica las funcionalidades principales del {concept}")
        
    if quality.actionability < 70:
        feedback_items.append(f"A√±ade verbos de acci√≥n espec√≠ficos para el {concept}")
        
    return {
        'feedback': '\n'.join([f"- {item}" for item in feedback_items])
    }

def generate_variations(prompt: str, model_name: str = "gpt2", num_variations: int = 3) -> Dict[str, Any]:
    """Funci√≥n de compatibilidad para generar variaciones"""
    model_manager = EnterpriseModelManager()
    improvement_engine = ProgressiveImprovementEngine(model_manager, AdvancedQualityAnalyzer())
    
    # Generar una mejora iterativa
    result = improvement_engine.improve_prompt_iteratively(
        prompt, 
        model_name, 
        max_iterations=2,
        target_quality=75.0
    )
    
    # Crear variaciones basadas en la mejora
    base_improved = result['final_prompt']
    variations = [base_improved]
    
    # Generar variaciones adicionales
    concept = extract_core_concept_enterprise(prompt)
    
    if len(variations) < num_variations:
        variations.append(f"{base_improved} con arquitectura moderna y escalable")
        
    if len(variations) < num_variations:
        variations.append(f"{base_improved} orientado a la experiencia del usuario")
        
    return {
        'improved_prompt': variations[0] if variations else prompt,
        'variations': variations[:num_variations]
    }

def generate_ideas(prompt: str, model_name: str = "gpt2", num_ideas: int = 3) -> Dict[str, Any]:
    """Funci√≥n de compatibilidad para generar ideas"""
    model_manager = EnterpriseModelManager()
    concept = extract_core_concept_enterprise(prompt)
    project_type = detect_project_type_enterprise(concept)
    
    # Generar ideas contextuales
    ideas = []
    
    if project_type == 'educacion':
        ideas = [
            f"Implementar sistema de gamificaci√≥n en el {concept}",
            f"A√±adir analytics de aprendizaje al {concept}",
            f"Integrar colaboraci√≥n en tiempo real en el {concept}"
        ]
    elif project_type == 'sistema':
        ideas = [
            f"Desarrollar API REST completa para el {concept}",
            f"Implementar dashboard administrativo en el {concept}",
            f"A√±adir sistema de notificaciones al {concept}"
        ]
    else:
        ideas = [
            f"Optimizar la experiencia de usuario del {concept}",
            f"Implementar funcionalidades avanzadas en el {concept}",
            f"A√±adir integraci√≥n con servicios externos al {concept}"
        ]
        
    return {
        'ideas': ideas[:num_ideas]
    }

def extract_core_concept_enterprise(prompt: str) -> str:
    """Extrae el concepto principal del prompt de forma empresarial"""
    prompt_lower = prompt.lower()
    
    # Patrones de conceptos comunes
    concept_patterns = {
        'sistema': r'sistema\s+(?:de\s+)?(\w+(?:\s+\w+)?)',
        'aplicacion': r'aplicaci[o√≥]n\s+(?:de\s+)?(\w+(?:\s+\w+)?)',
        'plataforma': r'plataforma\s+(?:de\s+)?(\w+(?:\s+\w+)?)',
        'web': r'(?:p√°gina|sitio)\s+web\s+(?:de\s+)?(\w+(?:\s+\w+)?)',
        'app': r'app\s+(?:de\s+)?(\w+(?:\s+\w+)?)'
    }
    
    for concept_type, pattern in concept_patterns.items():
        match = re.search(pattern, prompt_lower)
        if match:
            return f"{concept_type} {match.group(1)}"
            
    # Si no encuentra patr√≥n espec√≠fico, extraer palabras clave
    words = prompt.split()
    if len(words) >= 2:
        return f"{words[0]} {words[1]}"
        
    return "proyecto"

def detect_project_type_enterprise(concept: str) -> str:
    """Detecta el tipo de proyecto de forma empresarial"""
    concept_lower = concept.lower()
    
    if any(word in concept_lower for word in ['educacion', 'estudiante', 'curso', 'aprendizaje']):
        return 'educacion'
    elif any(word in concept_lower for word in ['sistema', 'gestion', 'administracion', 'empresa']):
        return 'sistema'
    elif any(word in concept_lower for word in ['web', 'sitio', 'pagina', 'portal']):
        return 'web'
    elif any(word in concept_lower for word in ['app', 'aplicacion', 'movil']):
        return 'aplicacion'
    else:
        return 'general'

def extract_keywords_enterprise(prompt: str) -> List[str]:
    """Extrae palabras clave de forma empresarial"""
    # Palabras clave t√©cnicas y de negocio
    technical_keywords = [
        'sistema', 'aplicacion', 'web', 'app', 'plataforma', 'api', 'base de datos',
        'frontend', 'backend', 'dashboard', 'interfaz', 'usuario', 'cliente',
        'gestion', 'administracion', 'reporte', 'analytics', 'seguridad'
    ]
    
    prompt_lower = prompt.lower()
    found_keywords = []
    
    for keyword in technical_keywords:
        if keyword in prompt_lower:
            found_keywords.append(keyword.title())
            
    # A√±adir palabras espec√≠ficas del dominio
    words = prompt.split()
    for word in words:
        if len(word) > 4 and word.lower() not in ['para', 'con', 'que', 'una', 'del', 'las', 'los']:
            if word.title() not in found_keywords:
                found_keywords.append(word.title())
                
    return found_keywords[:5]  # Limitar a 5 palabras clave

# Funci√≥n principal para testing
def main():
    """Funci√≥n principal para testing del sistema empresarial"""
    logger.info("üöÄ Iniciando PromptGen Enterprise System")
    
    # Inicializar componentes
    model_manager = EnterpriseModelManager()
    quality_analyzer = AdvancedQualityAnalyzer()
    improvement_engine = ProgressiveImprovementEngine(model_manager, quality_analyzer)
    
    # Prompt de prueba
    test_prompt = "Quiero crear una p√°gina web para una cafeter√≠a"
    
    logger.info(f"üìù Prompt de prueba: {test_prompt}")
    
    # Ejecutar mejora iterativa
    result = improvement_engine.improve_prompt_iteratively(
        test_prompt,
        model_name="gpt2",
        max_iterations=3,
        target_quality=80.0
    )
    
    # Mostrar resultados
    logger.info("üìä RESULTADOS FINALES:")
    logger.info(f"Prompt original: {result['original_prompt']}")
    logger.info(f"Prompt final: {result['final_prompt']}")
    logger.info(f"Mejora total: +{result['total_improvement']:.1f}%")
    logger.info(f"Iteraciones: {result['iterations_completed']}")
    
    return result

if __name__ == "__main__":
    main()