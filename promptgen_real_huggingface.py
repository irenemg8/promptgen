import os
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM
import warnings
import re
import time
import random
from typing import List, Dict, Tuple
import nltk
from nltk.corpus import stopwords
import spacy

# Descargar recursos necesarios
try:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
except:
    pass

# Ignorar advertencias
warnings.filterwarnings("ignore")

# Cargar modelo de spaCy para espa√±ol (para procesamiento inteligente)
try:
    nlp = spacy.load("es_core_news_sm")
except:
    print("‚ö†Ô∏è Modelo spaCy no encontrado. Instalando...")
    os.system("python -m spacy download es_core_news_sm")
    nlp = spacy.load("es_core_news_sm")

# --- CONFIGURACI√ìN REAL ---

# Diccionario de traducci√≥n de conceptos comunes mal generados
COMMON_FIXES = {
    # Palabras mal generadas -> correcci√≥n
    '√±√≥n': 'con',
    '√°retera': 'carretera',
    'v√©ritima': 'mar√≠tima',
    'comercionado': 'comercializado',
    'ciencias': 'ciencias',
    'enfranco': 'enfoque',
    'llev√°dica': 'llevada',
    'espaol': 'espa√±ol',
    'hist√≥ria': 'historia',
    't√©preca': 't√©cnica',
    'n√≠tos': 'ni√±os',
    'loridad': 'claridad',
    'como√±os': 'comunes',
    'estuaron': 'estudiaron',
    'peruar': 'peruar',
    'enlaceraci√≥n': 'enlace',
    'noch√°xico': 'nocturno',
    's√≥dica': 's√≥lida'
}

# Patrones de extracci√≥n inteligente
EXTRACTION_PATTERNS = {
    'features': [
        r'con\s+(\w+(?:\s+\w+)?)\s+(?:y|,)',
        r'incluye\s+(\w+(?:\s+\w+)?)',
        r'sistema\s+de\s+(\w+(?:\s+\w+)?)',
        r'm√≥dulo\s+de\s+(\w+(?:\s+\w+)?)',
        r'(\w+(?:\s+\w+)?)\s+avanzad[oa]',
        r'(\w+(?:\s+\w+)?)\s+profesional',
        r'tecnolog√≠a\s+de\s+(\w+(?:\s+\w+)?)'
    ],
    'actions': [
        r'(desarrolla|crea|dise√±a|implementa|genera|construye)',
        r'para\s+(mejorar|optimizar|facilitar|automatizar)',
        r'que\s+(permite|facilita|ayuda|mejora)'
    ],
    'targets': [
        r'para\s+(\w+(?:\s+\w+)?)',
        r'dirigido\s+a\s+(\w+(?:\s+\w+)?)',
        r'usuarios?\s+(\w+(?:\s+\w+)?)',
        r'clientes?\s+(\w+(?:\s+\w+)?)'
    ]
}

# Templates de mejora basados en extracci√≥n
IMPROVEMENT_RULES = {
    'add_specificity': [
        "con funcionalidades espec√≠ficas de {extracted_feature}",
        "incluyendo sistema avanzado de {extracted_feature}",
        "con m√≥dulo profesional de {extracted_feature}"
    ],
    'add_technology': [
        "usando tecnolog√≠a de {tech_suggestion}",
        "implementado con {tech_suggestion}",
        "basado en arquitectura {tech_suggestion}"
    ],
    'add_audience': [
        "orientado a {audience_type}",
        "dise√±ado para {audience_type}",
        "optimizado para {audience_type}"
    ]
}

# Cache de modelos
model_cache = {}
iteration_history = {}  # Para tracking de mejoras reales

def load_model_pipeline(model_name):
    """Carga REAL del modelo de Hugging Face"""
    if model_name in model_cache:
        return model_cache[model_name]
    
    print(f"üîÑ Cargando modelo REAL {model_name}...")
    start_time = time.time()
    
    try:
        # Nombres correctos de modelos
        model_map = {
            "gpt2": "gpt2",
            "distilgpt2": "distilgpt2",
            "t5-small": "google/t5-v1_1-small",
            "gpt-neo-125m": "EleutherAI/gpt-neo-125M"
        }
        
        actual_model_name = model_map.get(model_name, model_name)
        
        tokenizer = AutoTokenizer.from_pretrained(actual_model_name, trust_remote_code=True)
        
        if "t5" in actual_model_name.lower():
            model = AutoModelForSeq2SeqLM.from_pretrained(actual_model_name, trust_remote_code=True)
            pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
        else:
            model = AutoModelForCausalLM.from_pretrained(actual_model_name, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                model.config.pad_token_id = model.config.eos_token_id
            pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
        
        model_cache[model_name] = pipe
        load_time = time.time() - start_time
        print(f"‚úÖ Modelo {model_name} cargado REALMENTE en {load_time:.1f}s")
        return pipe
        
    except Exception as e:
        print(f"‚ùå Error cargando {model_name}: {e}")
        return None

def generate_raw_with_model(model_name, prompt, max_length=100):
    """Genera texto RAW con el modelo REAL"""
    pipe = load_model_pipeline(model_name)
    if not pipe:
        return None
    
    print(f"ü§ñ Generando con {model_name} REAL...")
    start_time = time.time()
    
    try:
        if "t5" in model_name.lower():
            # T5 necesita tareas espec√≠ficas
            task_prompt = f"paraphrase: {prompt}"
            result = pipe(
                task_prompt,
                max_length=max_length,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.9,
                top_p=0.95
            )
            raw_output = result[0]['generated_text']
        else:
            # GPT-2 y similares
            result = pipe(
                prompt,
                max_new_tokens=max_length,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.9,
                top_p=0.95,
                pad_token_id=pipe.tokenizer.eos_token_id,
                repetition_penalty=1.2
            )
            raw_output = result[0]['generated_text']
            # Remover el prompt original
            if raw_output.startswith(prompt):
                raw_output = raw_output[len(prompt):].strip()
        
        generation_time = time.time() - start_time
        print(f"‚è±Ô∏è Generado en {generation_time:.1f}s")
        print(f"üìù Salida RAW: {raw_output[:100]}...")
        
        return raw_output
        
    except Exception as e:
        print(f"‚ùå Error generando: {e}")
        return None

def intelligent_spanish_processing(raw_text, original_prompt, task_type="improve"):
    """Procesamiento INTELIGENTE para convertir salida basura en espa√±ol √∫til"""
    if not raw_text:
        return None
    
    print(f"üß† Procesando inteligentemente la salida...")
    
    # Paso 1: Limpieza b√°sica y correcci√≥n de palabras conocidas
    cleaned_text = raw_text.lower()
    for wrong, correct in COMMON_FIXES.items():
        cleaned_text = cleaned_text.replace(wrong, correct)
    
    # Paso 2: Extraer palabras/frases en espa√±ol v√°lidas
    spanish_words = []
    words = cleaned_text.split()
    
    for word in words:
        # Verificar si es una palabra espa√±ola v√°lida (tiene vocales espa√±olas)
        if re.search(r'[aeiou√°√©√≠√≥√∫√±]', word) and len(word) > 2:
            # Limpiar caracteres extra√±os
            clean_word = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º\s]', '', word)
            if clean_word and len(clean_word) > 2:
                spanish_words.append(clean_word)
    
    # Paso 3: Extraer conceptos √∫tiles usando patrones
    extracted_features = []
    
    # Buscar patrones de caracter√≠sticas
    for pattern_type, patterns in EXTRACTION_PATTERNS.items():
        for pattern in patterns:
            matches = re.findall(pattern, cleaned_text)
            for match in matches:
                if isinstance(match, str) and len(match) > 3:
                    extracted_features.append(match)
    
    # Paso 4: Construir respuesta inteligente basada en la tarea
    concept = extract_core_concept(original_prompt)
    
    if task_type == "improve":
        # Usar caracter√≠sticas extra√≠das para mejorar
        if extracted_features:
            feature = random.choice(extracted_features)
            improvement = f"{concept} profesional con sistema avanzado de {feature}"
        else:
            # Generar mejora basada en an√°lisis del prompt original
            missing = analyze_what_is_missing(original_prompt)
            improvement = f"{concept} completo que incluye {missing}"
        return improvement
    
    elif task_type == "feedback":
        # Generar feedback basado en lo que falta
        missing_elements = analyze_what_is_missing_detailed(original_prompt)
        return missing_elements
    
    elif task_type == "ideas":
        # Generar ideas basadas en caracter√≠sticas extra√≠das
        if extracted_features:
            ideas = []
            for i, feature in enumerate(extracted_features[:3]):
                idea = f"Implementar m√≥dulo de {feature} avanzado en el {concept}"
                ideas.append(idea)
            return ideas
        else:
            # Ideas basadas en an√°lisis
            return generate_contextual_ideas_from_analysis(concept, original_prompt)
    
    return None

def analyze_what_is_missing(prompt):
    """Analiza qu√© le falta al prompt"""
    prompt_lower = prompt.lower()
    missing = []
    
    if 'usuario' not in prompt_lower and 'para' not in prompt_lower:
        missing.append("definici√≥n de usuarios objetivo")
    if 'tecnolog√≠a' not in prompt_lower and 'sistema' not in prompt_lower:
        missing.append("arquitectura t√©cnica")
    if 'objetivo' not in prompt_lower and 'prop√≥sito' not in prompt_lower:
        missing.append("objetivos espec√≠ficos")
    if len(prompt.split()) < 10:
        missing.append("descripci√≥n detallada")
    
    return ", ".join(missing[:2]) if missing else "funcionalidades avanzadas y m√©tricas de √©xito"

def analyze_what_is_missing_detailed(prompt):
    """An√°lisis detallado para feedback"""
    prompt_lower = prompt.lower()
    concept = extract_core_concept(prompt)
    feedback = []
    
    # An√°lisis contextual real
    if 'usuario' not in prompt_lower and 'audiencia' not in prompt_lower:
        feedback.append(f"Define el tipo de usuarios objetivo para el {concept}")
    
    if 'funcionalidad' not in prompt_lower and 'caracter√≠stica' not in prompt_lower:
        feedback.append(f"Especifica las funcionalidades principales del {concept}")
    
    if 'tecnolog√≠a' not in prompt_lower and 'implementar' not in prompt_lower:
        feedback.append(f"Menciona la tecnolog√≠a o plataforma para implementar el {concept}")
    
    if len(prompt.split()) < 8:
        feedback.append(f"Proporciona m√°s detalles espec√≠ficos sobre el {concept}")
    
    if 'objetivo' not in prompt_lower and 'prop√≥sito' not in prompt_lower:
        feedback.append(f"Establece los objetivos claros del {concept}")
    
    return feedback[:3] if feedback else [f"Define casos de uso espec√≠ficos para el {concept}"]

def generate_contextual_ideas_from_analysis(concept, prompt):
    """Genera ideas contextuales basadas en an√°lisis"""
    ideas = []
    
    # Detectar tipo de proyecto
    if any(word in prompt.lower() for word in ['historia', 'cuento', 'escritor', 'generador']):
        ideas = [
            f"A√±adir sistema de templates personalizables al {concept}",
            f"Implementar an√°lisis de estilo de escritura en el {concept}",
            f"Crear biblioteca de g√©neros y estilos para el {concept}"
        ]
    elif any(word in prompt.lower() for word in ['sistema', 'gesti√≥n', 'empresa']):
        ideas = [
            f"Implementar dashboard anal√≠tico en tiempo real para el {concept}",
            f"A√±adir m√≥dulo de reportes automatizados al {concept}",
            f"Crear API REST para integraci√≥n del {concept} con otros sistemas"
        ]
    else:
        ideas = [
            f"Desarrollar versi√≥n m√≥vil responsive del {concept}",
            f"A√±adir sistema de notificaciones inteligentes al {concept}",
            f"Implementar anal√≠ticas y m√©tricas de uso en el {concept}"
        ]
    
    return ideas

def extract_core_concept(prompt):
    """Extrae el concepto principal del prompt"""
    # Usar spaCy para an√°lisis m√°s inteligente
    try:
        doc = nlp(prompt.lower())
        # Buscar sustantivos principales
        nouns = [token.text for token in doc if token.pos_ == "NOUN"]
        if nouns:
            # Tomar los primeros 2-3 sustantivos significativos
            concept_words = []
            for noun in nouns:
                if len(noun) > 2 and noun not in ['cosa', 'tipo']:
                    concept_words.append(noun)
                if len(concept_words) >= 3:
                    break
            return ' '.join(concept_words) if concept_words else "proyecto"
    except:
        pass
    
    # Fallback: m√©todo simple
    words = prompt.lower().split()
    filtered = [w for w in words if len(w) > 3 and w not in ['para', 'crear', 'hacer', 'desarrollar']]
    return ' '.join(filtered[:3]) if filtered else "proyecto"

def progressive_improvement(prompt, iteration, model_name):
    """Mejora PROGRESIVA real usando el modelo"""
    print(f"\nüîÑ Iteraci√≥n {iteration} - Mejora progresiva con {model_name}")
    
    # Generar prompt de mejora espec√≠fico para la iteraci√≥n
    improvement_prompts = [
        f"Mejora este concepto a√±adiendo detalles t√©cnicos: {prompt}",
        f"Expande esta idea con funcionalidades espec√≠ficas: {prompt}",
        f"Optimiza este proyecto a√±adiendo arquitectura: {prompt}",
        f"Perfecciona esta propuesta con tecnolog√≠as modernas: {prompt}",
        f"Finaliza este dise√±o con m√©tricas de √©xito: {prompt}"
    ]
    
    improvement_prompt = improvement_prompts[min(iteration-1, len(improvement_prompts)-1)]
    
    # Generar con el modelo REAL
    raw_output = generate_raw_with_model(model_name, improvement_prompt, max_length=80)
    
    if raw_output:
        # Procesar inteligentemente
        improved = intelligent_spanish_processing(raw_output, prompt, "improve")
        
        if improved and improved != prompt:
            # Verificar que realmente es una mejora
            if len(improved.split()) > len(prompt.split()) or any(word in improved for word in ['profesional', 'avanzado', 'completo']):
                return improved
    
    # Fallback: mejora incremental basada en an√°lisis
    missing = analyze_what_is_missing(prompt)
    return f"{prompt} con {missing}"

# --- FUNCIONES DE API REALES ---

def analyze_prompt_quality_bart(prompt: str) -> Dict:
    """An√°lisis de calidad REAL usando el concepto extra√≠do"""
    concept = extract_core_concept(prompt)
    word_count = len(prompt.split())
    
    # An√°lisis real de elementos presentes
    has_objective = any(word in prompt.lower() for word in ['para', 'objetivo', 'prop√≥sito'])
    has_audience = any(word in prompt.lower() for word in ['usuario', 'cliente', 'dirigido', 'audiencia'])
    has_features = any(word in prompt.lower() for word in ['con', 'incluye', 'funcionalidad', 'caracter√≠stica'])
    has_tech = any(word in prompt.lower() for word in ['tecnolog√≠a', 'sistema', 'plataforma', 'aplicaci√≥n'])
    
    # Puntuaci√≥n basada en an√°lisis real
    completeness = 40 + (10 if has_objective else 0) + (10 if has_audience else 0) + (10 if has_features else 0) + (10 if has_tech else 0) + min(word_count * 2, 30)
    clarity = 60 + (20 if 8 <= word_count <= 30 else 0) + (10 if has_objective else 0)
    specificity = 50 + (15 if has_features else 0) + (15 if has_tech else 0) + (10 if has_audience else 0)
    structure = 70 + (15 if any(verb in prompt.lower() for verb in ['desarrolla', 'crea', 'dise√±a']) else 0)
    
    # Asegurar l√≠mites
    completeness = min(100, completeness)
    clarity = min(100, clarity)
    specificity = min(100, specificity)
    structure = min(100, structure)
    
    overall_score = round((completeness + clarity + specificity + structure) / 4)
    
    # Determinar tipo de proyecto
    project_type = "General"
    if any(word in prompt.lower() for word in ['historia', 'cuento', 'escritor']):
        project_type = "Creativo"
    elif any(word in prompt.lower() for word in ['sistema', 'gesti√≥n', 'empresa']):
        project_type = "Sistema"
    elif any(word in prompt.lower() for word in ['educativo', 'tutorial', 'curso']):
        project_type = "Educativo"
    
    # Generar reporte
    status = "üèÜ Excelente" if overall_score >= 90 else "‚úÖ Muy Buena" if overall_score >= 80 else "‚úÖ Buena" if overall_score >= 70 else "‚ö†Ô∏è Aceptable" if overall_score >= 60 else "‚ùå Necesita Mejoras"
    
    report = f"""üìä An√°lisis detallado del prompt ({word_count} palabras)

{status.split()[0]} Calidad general: {overall_score}% - {' '.join(status.split()[1:])}
üéØ Tipo de proyecto detectado: {project_type}
üîë Concepto principal: {concept}

üìà An√°lisis por categor√≠as:
‚Ä¢ Completitud: {completeness}%
‚Ä¢ Claridad: {clarity}%
‚Ä¢ Especificidad: {specificity}%
‚Ä¢ Estructura: {structure}%"""

    if overall_score < 90:
        feedback_items = analyze_what_is_missing_detailed(prompt)
        if feedback_items:
            report += "\n\nüí° Recomendaciones de mejora:\n"
            for i, item in enumerate(feedback_items, 1):
                report += f"{i}. {item}\n"
    
    # Palabras clave reales extra√≠das
    try:
        doc = nlp(prompt)
        keywords = [token.text for token in doc if token.pos_ in ["NOUN", "VERB"] and len(token.text) > 3]
        keywords_str = ", ".join(list(set(keywords))[:4])
    except:
        keywords_str = concept
    
    return {
        "quality_report": report,
        "interpreted_keywords": keywords_str,
        "raw_scores": {
            "completeness": completeness,
            "clarity": clarity,
            "specificity": specificity,
            "structure": structure
        }
    }

def get_structural_feedback(prompt: str, model_name: str = "gpt2") -> Dict:
    """Feedback estructural REAL usando el modelo"""
    # Generar feedback usando el modelo
    feedback_prompt = f"Analiza qu√© mejorar en: {prompt}"
    raw_output = generate_raw_with_model(model_name, feedback_prompt, max_length=100)
    
    # Procesar para obtener feedback √∫til
    if raw_output:
        feedback_items = intelligent_spanish_processing(raw_output, prompt, "feedback")
    else:
        feedback_items = analyze_what_is_missing_detailed(prompt)
    
    feedback_text = "\n".join([f"- {item}" for item in feedback_items])
    
    return {"feedback": feedback_text}

def generate_variations(prompt: str, model_name: str = "gpt2", num_variations: int = 3) -> Dict:
    """Genera variaciones REALES usando el modelo"""
    print(f"\nüîÑ Generando {num_variations} variaciones REALES con {model_name}")
    
    variations = []
    
    for i in range(num_variations):
        # Diferentes prompts para variaci√≥n
        variation_prompts = [
            f"Versi√≥n mejorada y detallada de: {prompt}",
            f"Reescribe profesionalmente: {prompt}",
            f"Optimiza y expande: {prompt}"
        ]
        
        var_prompt = variation_prompts[i % len(variation_prompts)]
        
        # Generar con modelo REAL
        raw_output = generate_raw_with_model(model_name, var_prompt, max_length=60)
        
        if raw_output:
            # Procesar inteligentemente
            processed = intelligent_spanish_processing(raw_output, prompt, "improve")
            if processed and processed not in variations:
                variations.append(processed)
            else:
                # Si falla, usar mejora progresiva
                improved = progressive_improvement(prompt, i+1, model_name)
                variations.append(improved)
        else:
            # Fallback
            improved = progressive_improvement(prompt, i+1, model_name)
            variations.append(improved)
        
        time.sleep(0.3)
    
    return {"variations": variations}

def generate_ideas(prompt: str, model_name: str = "gpt2", num_ideas: int = 3) -> Dict:
    """Genera ideas REALES usando el modelo"""
    print(f"\nüß† Generando ideas REALES con {model_name}")
    
    # Generar ideas con el modelo
    ideas_prompt = f"Ideas innovadoras para mejorar: {prompt}"
    raw_output = generate_raw_with_model(model_name, ideas_prompt, max_length=150)
    
    if raw_output:
        ideas = intelligent_spanish_processing(raw_output, prompt, "ideas")
    else:
        concept = extract_core_concept(prompt)
        ideas = generate_contextual_ideas_from_analysis(concept, prompt)
    
    return {"ideas": ideas[:num_ideas]}

def test_iterative_improvement(initial_prompt: str, model_name: str = "gpt2", iterations: int = 5):
    """Test de mejora iterativa REAL"""
    print(f"\n{'='*60}")
    print(f"üß™ TEST DE MEJORA ITERATIVA REAL")
    print(f"üìù Prompt inicial: '{initial_prompt}'")
    print(f"ü§ñ Modelo: {model_name}")
    print(f"üîÑ Iteraciones: {iterations}")
    print(f"{'='*60}")
    
    current_prompt = initial_prompt
    history = []
    
    for i in range(1, iterations + 1):
        print(f"\n--- Iteraci√≥n {i} ---")
        
        # An√°lisis de calidad
        analysis = analyze_prompt_quality_bart(current_prompt)
        scores = analysis['raw_scores']
        overall = round(sum(scores.values()) / len(scores))
        
        print(f"üìä Calidad actual: {overall}%")
        print(f"üìù Prompt: '{current_prompt}'")
        
        # Generar mejora
        variations = generate_variations(current_prompt, model_name, 1)
        improved_prompt = variations['variations'][0]
        
        print(f"‚ú® Mejorado a: '{improved_prompt}'")
        
        history.append({
            'iteration': i,
            'prompt': current_prompt,
            'score': overall,
            'improved': improved_prompt
        })
        
        current_prompt = improved_prompt
        
        # Si llegamos a 90% o m√°s, objetivo logrado
        if overall >= 90:
            print(f"\nüéâ ¬°Objetivo alcanzado! Calidad: {overall}%")
            break
    
    # Resumen
    print(f"\n{'='*60}")
    print(f"üìà RESUMEN DE EVOLUCI√ìN:")
    for h in history:
        print(f"   Iteraci√≥n {h['iteration']}: {h['score']}% -> {h['prompt'][:50]}...")
    print(f"{'='*60}")
    
    return history

def main():
    print("üöÄ PromptGen REAL con Hugging Face")
    print("‚úÖ Usa REALMENTE los modelos (sin mockups)")
    print("‚úÖ Procesa inteligentemente las salidas")
    print("‚úÖ Mejora progresiva real en cada iteraci√≥n")
    print("‚úÖ Convierte basura en espa√±ol √∫til")
    
    # Instalar dependencias si no est√°n
    try:
        import spacy
        import nltk
    except:
        print("üì¶ Instalando dependencias necesarias...")
        os.system("pip install spacy nltk")
        os.system("python -m spacy download es_core_news_sm")

if __name__ == '__main__':
    main() 