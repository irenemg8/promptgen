# ğŸ¤– PromptGen - Sistema de Chat con Documentos

## ğŸ“‹ Resumen

Se ha aÃ±adido una nueva funcionalidad de **chat con documentos** que permite:

- âœ… **Subir documentos** (PDF, Word, TXT) y almacenarlos localmente
- âœ… **Procesamiento automÃ¡tico** con embeddings para bÃºsqueda semÃ¡ntica
- âœ… **Chat inteligente** que responde preguntas basÃ¡ndose en el contenido de los documentos
- âœ… **Almacenamiento persistente** local (sin envÃ­o a APIs externas)
- âœ… **Interfaz moderna** con historial de conversaciones y fuentes citadas

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Instalar Dependencias Python

```bash
# Instalar las nuevas dependencias
pip install -r requirements.txt

# Dependencias principales aÃ±adidas:
# - ollama (cliente para Ollama)
# - chromadb (base de datos vectorial)
# - langchain (framework para RAG)
# - pypdf2 (procesamiento de PDFs)
# - python-docx (procesamiento de Word)
```

### 2. Configurar Ollama

#### OpciÃ³n A: InstalaciÃ³n AutomÃ¡tica
```bash
# Ejecutar script de configuraciÃ³n automÃ¡tica
python setup_ollama.py
```

#### OpciÃ³n B: InstalaciÃ³n Manual

**Windows:**
1. Descarga Ollama desde: https://ollama.com/download
2. Ejecuta el instalador
3. Abre CMD/PowerShell y ejecuta:
```bash
ollama pull llama3.2:3b
ollama pull mxbai-embed-large
```

**macOS/Linux:**
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelos
ollama pull llama3.2:3b
ollama pull mxbai-embed-large
```

### 3. Verificar InstalaciÃ³n

```bash
# Verificar que Ollama estÃ¡ ejecutÃ¡ndose
ollama list

# Iniciar servidor si no estÃ¡ corriendo
ollama serve

# Verificar API (deberÃ­a devolver JSON con modelos)
curl http://localhost:11434/api/tags
```

## ğŸš€ Uso del Sistema

### 1. Iniciar AplicaciÃ³n

```bash
# Terminal 1: Backend (API)
python api_server.py

# Terminal 2: Frontend (Next.js)
npm run dev

# Terminal 3: Ollama (si no estÃ¡ como servicio)
ollama serve
```

### 2. Acceder al Chat

1. Visita: http://localhost:3000
2. Haz clic en el botÃ³n **"Chat"** en el header
3. O accede directamente a: http://localhost:3000/chat

### 3. Subir Documentos

1. **Seleccionar archivo:** Haz clic en "Seleccionar Archivo"
2. **Formatos soportados:** PDF, Word (.docx, .doc), TXT
3. **Subir:** Haz clic en "Subir y Procesar"
4. **Esperar:** El sistema procesarÃ¡ el documento automÃ¡ticamente
5. **ConfirmaciÃ³n:** VerÃ¡s un mensaje de Ã©xito en el chat

### 4. Hacer Consultas

1. **Escribir pregunta:** En el campo de texto del chat
2. **Enviar:** Presiona Enter o haz clic en el botÃ³n de enviar
3. **Respuesta:** El sistema buscarÃ¡ en los documentos y generarÃ¡ una respuesta
4. **Fuentes:** VerÃ¡s las fuentes citadas debajo de cada respuesta

## ğŸ“‚ Estructura de Archivos

```
promptgen/
â”œâ”€â”€ documents_storage/          # Almacenamiento local de documentos
â”‚   â”œâ”€â”€ uploads/               # Archivos subidos
â”‚   â”œâ”€â”€ vectordb/              # Base de datos vectorial (ChromaDB)
â”‚   â””â”€â”€ documents_metadata.json # Metadatos de documentos
â”œâ”€â”€ document_rag_system.py      # Sistema RAG principal
â”œâ”€â”€ setup_ollama.py            # Script de configuraciÃ³n
â”œâ”€â”€ components/chat-interface.tsx # Componente de chat
â”œâ”€â”€ app/chat/page.tsx          # PÃ¡gina de chat
â””â”€â”€ api_server.py              # Backend con endpoints RAG
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

```bash
# Configurar modelos (opcional)
OLLAMA_MODEL=llama3.2:3b
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large

# Configurar paths (opcional)
DOCUMENTS_STORAGE_PATH=./documents_storage
VECTOR_DB_PATH=./documents_storage/vectordb
```

### PersonalizaciÃ³n de Modelos

Puedes cambiar los modelos editando `document_rag_system.py`:

```python
# Cambiar modelo de chat
model_name = "llama3.1:8b"  # Modelo mÃ¡s grande pero mÃ¡s lento

# Cambiar modelo de embeddings
embeddings_model = "nomic-embed-text"  # Alternativa ligera
```

## ğŸ¯ Funcionalidades Disponibles

### Chat Inteligente
- âœ… Responde preguntas basÃ¡ndose en documentos subidos
- âœ… Cita fuentes especÃ­ficas
- âœ… Mantiene contexto conversacional
- âœ… Manejo de errores y respuestas informativas

### GestiÃ³n de Documentos
- âœ… Subida con drag & drop
- âœ… Vista previa de archivos seleccionados
- âœ… Lista de documentos procesados
- âœ… EliminaciÃ³n de documentos
- âœ… Conteo de fragmentos procesados

### Procesamiento Inteligente
- âœ… DivisiÃ³n automÃ¡tica en fragmentos (chunks)
- âœ… Embeddings semÃ¡nticos
- âœ… BÃºsqueda por similitud
- âœ… Metadatos enriquecidos

## ğŸ“Š Endpoints API

### Chat y Consultas
- `POST /api/query` - Realizar consulta sobre documentos
- `POST /api/upload_document` - Subir y procesar documento
- `GET /api/documents` - Obtener lista de documentos
- `DELETE /api/documents/{id}` - Eliminar documento
- `GET /api/rag_status` - Estado del sistema RAG

### Ejemplo de Uso API

```bash
# Subir documento
curl -X POST -F "file=@documento.pdf" http://localhost:8000/api/upload_document

# Hacer consulta
curl -X POST -H "Content-Type: application/json" \
  -d '{"query": "Â¿CuÃ¡l es el tema principal?", "k": 5}' \
  http://localhost:8000/api/query

# Listar documentos
curl http://localhost:8000/api/documents
```

## ğŸ›¡ï¸ Seguridad y Privacidad

### Procesamiento Local
- âœ… **Sin APIs externas:** Todo el procesamiento es local
- âœ… **Datos privados:** Los documentos no salen de tu mÃ¡quina
- âœ… **Offline:** Funciona sin conexiÃ³n a internet
- âœ… **Control total:** TÃº controlas tus datos

### Almacenamiento Seguro
- âœ… **EncriptaciÃ³n:** Base de datos vectorial local
- âœ… **Metadatos:** InformaciÃ³n estructurada en JSON
- âœ… **Cleanup:** Funcionalidad de eliminaciÃ³n completa

## ğŸ” SoluciÃ³n de Problemas

### Ollama no se conecta
```bash
# Verificar que Ollama estÃ© corriendo
ollama ps

# Iniciar manualmente
ollama serve

# Verificar puerto
netstat -an | grep 11434
```

### Modelos no encontrados
```bash
# Listar modelos instalados
ollama list

# Descargar modelos faltantes
ollama pull llama3.2:3b
ollama pull mxbai-embed-large
```

### Error de memoria
```bash
# Usar modelo mÃ¡s pequeÃ±o
ollama pull llama3.2:1b

# Verificar memoria disponible
free -h  # Linux
vm_stat  # macOS
```

### Archivos no se procesan
- âœ… Verificar formato soportado (PDF, DOCX, TXT)
- âœ… Verificar tamaÃ±o del archivo (< 50MB recomendado)
- âœ… Verificar permisos de la carpeta documents_storage/
- âœ… Revisar logs del servidor para errores especÃ­ficos

## ğŸ“ˆ Rendimiento

### Recomendaciones de Hardware
- **CPU:** 4+ cores para procesamiento eficiente
- **RAM:** 8GB mÃ­nimo, 16GB recomendado
- **Almacenamiento:** SSD para mejor rendimiento de la base de datos vectorial

### Optimizaciones
- **TamaÃ±o de chunks:** Ajustar chunk_size en document_rag_system.py
- **NÃºmero de resultados:** Modificar parÃ¡metro k en las consultas
- **Modelo mÃ¡s rÃ¡pido:** Usar llama3.2:1b para respuestas mÃ¡s rÃ¡pidas

## ğŸ¤ Contribuir

### AÃ±adir nuevos formatos
1. Editar `load_document()` en `document_rag_system.py`
2. AÃ±adir loader apropiado de langchain
3. Actualizar `allowed_types` en el API

### Mejorar embeddings
1. Cambiar modelo de embeddings en la configuraciÃ³n
2. Probar con diferentes modelos de Ollama
3. Ajustar parÃ¡metros de similitud

## ğŸ“ Soporte

Para problemas tÃ©cnicos o consultas:

- **Email:** irenebati4@gmail.com
- **Issues:** Crear issue en el repositorio
- **DocumentaciÃ³n:** Revisar ollama_config.md

Â¡Disfruta del nuevo sistema de chat con documentos! ğŸ‰ 